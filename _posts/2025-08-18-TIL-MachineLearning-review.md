---
layout: post
title: "8월 18일 월요일 TIL(MachineLearning-review)"
date: 2025-08-18 18:00:00 +0900
categories: July_week8
---

# 8월 18일 월요일 TIL 작성

## 1. 학습 주제


## 2. 새롭게 알게된 점 / 핵심 개념 (가장 중요하다고 생각하는 개념)
### 머신러닝, 딥러닝 기초

퍼셉트론 : 데이터에 가중치를 곱하고 그것을 활성함수를 통해서 결과를 도출, 가중치는 예측과 실제 결과의 오차를 기반으로 수정

→ 그러나 XOR 문제를 해결하지 못함. 따라서 다층 퍼셉트론이라는 개념이 나옴

다층 퍼셉트론 : 은닉층을 추가하기, 데이터 예측 함수를 1개 만드는 것이 아니라, 여러 개를 만드는 것으로  XOR 문제를 해결. 

은닉칭이 점점 깊어지고, 커짐. + 오차 역전파를 통해서 우리가 알기 어려운 은닉층의 가중치를 수정한다.

순전파 : 가지고 있는 가중치와 데이터를 기반으로 예측 값을 출력하기

역전파 : 예측 값과 실제값의 오차를 기반으로 편미분을 통해서 역방향으로 은닉층 가중치를 수정

그러나 은닉층의 깊이가 깊어질 수록, 편미분이 여러 번 발생하여, 앞 쪽으로 갈 수록 기울기 소실이 발생. 제대로 된 학습이 발생하지 않는다. 이 문제는 활성화함수로 **시그모이드를** 사용했기 떄문에

따라서 다른 활성화 함수를 만듬. 그중에 ReLU 함수를 많이 사용함. (그러나 기울기 소실 문제가 완전히 해결 된 것은 아님)

또한 경사 하강법 또한 속도와 정확도를 높이기 위해서 **Adam 이라는 경사 하강법**을 사용한다. 

### 모델 만들기

모델은 keras의 Sequential() 을 사용, 활성함수로 ‘relu’ 사용.

마지막 활성함수는 분류모델일 경우 0~1 사이의 확률로 리턴하기 위해서 ‘sigmoid’ 함수를 사용한다. 또한 compile에서 loss=’binary_crossentropy’ 를 사용한다. optimizer=’adam’을 사용한다(오차를 줄이기 위해서), loss 함수는 상황에 따라서 여러 방식을 사용할 수 있다.

실제로 모델을 학습시키는 함수는 fit() 이다. 그 안에 epochs(학습 수), batch_size(한 번에 학습할 데이터 수) 매개변수를 넣어줄 수 있다.

### 판다스로 데이터 분석과 모델에 적용하기

df[].value_counts() 를 통해서 특정 값이 몇 개 씩 있는지를 알 수 있다.

df.corr() 를 통해서 상관 관계 분석을 할 수 있다. 또한 이를 히트맵으로 그릴 수 있다.

우리가 분석한 데이터를 학습을 할 X와 정답인 y로 분리한다. → 그러나 정답을 모두 알고 있기에 과적합이 발생한다.

### 다중 분류 문제

3개 이상의 라벨로 분류하는 문제

속성에서 카테코리컬한 것이 있다면, 그것을 **원 핫 인코딩** 을 통해서 숫자로 분리해서 표현한다.

다중 분류 문제는 활성 함수가 softmax이다. 이는 여러 분류 중 가장 확률이 높은 것을 고를 때 사용하는데, softmax는 총합 1을 기반으로 각 분류별로 확률이 어느정도 될지를 계산한다. 자연어처리의 트랜스포머도, 디코더의 마지막은 softmax로 구성한다. 문장을 작성할 때 다음에 올 단어들 중 가장 확률이 높은 것을 골라아 하기 떄문에

### 모델 성능 검증하기

학습한 데이터를 기반으로 테스트를 하면 어느 순간 정확도가 100%가 되는 것, **과적합** 을 확인할 수 있다.  혹은 은닉층이 너무 깊거나, 여러 이유로 과적합 발생 가능 → 일반적인 상황의 추론 성능이 오히려 떨어진다.

학습 셋과 테스트 셋을 분리하자. 사실 정확히는 학습셋과 평가 셋을 분리하자고 할 수도 있을 것 같다. 내 생각에 테스트 셋은 우리가 실제 모델을 사용할 때 들어갈 데이터라고 생각한다. 우리가 진짜 알고자 하는 데이터에 대해서.

*k겹 교차 검증*. 전체 데이터에 대해서 겹치지 않게 여러 번 학습데이터와 평가 데이터를 분리.

검증 셋 : 학습 중에 모델을 평가하기 위한 데이터, 학습 데이터에 포함되지 않아서 정답을 알고 있지 않고, 모델이 학습 데이터에 과적합이 되고있지 않은지 확인할 수 있는 데이터 셋

model.fit에서 validation_split=0.2 와 같은 것을 통해서 들어온 학습데이터의 20% 를 학습에 사용하지 않고 검증 데이터로 활용한다.

또한 callback 함수를 통해서 과적합이 발생하는 경우 학습을 중단할 수 있다. EarlyStopping 을 통해서 기능을 쓸 수 있다.

### CNN

이미지에 최적화된 모델, 입력된 이미지에서 다시 한 번 특징을 추출하기 위해서 커널(슬라이딩 윈도우)을 도입하는 기법.

커널을 통해서 이미지의 사이즈를 줄이고 전체적인 특징을 뽑아낸다. 그리고 여러 커널을 이용해서 각 커널 별로 특정한 특징을 찾아낼 수 있다.

이렇게 커널로 처리를 한 이후로는 1차원으로 쭉 뽑아낸 이후(Flatten) Dense layer 에 넣고 softmax 등에 넣는다.

맥스 풀링, 드롭아웃, 등을 통해서 과적합을 방지할 수 있다.

### 자연어 처리

word embedding :  단어를 원 핫 인코딩으로 처리하면, 벡터의 길이가 너무 길어지고, 각 단어간의 유사성을 파악 할 수 없게 된다.

따라서 유사도를 파악할 수 있는 특성등을 바탕으로 그 차원공간에 단어를 배치하는 것으로 벡터의 크기를 줄이고, 단어의 의미적, 문법적 유사도(관련도)를 계산하고 파악할 수 있다.

즉, 밀집된 차원으로 표현하는 것으로 계산 효율성을 높이고, 그 벡터 공간안이 배치로 단어간 유사성을 파악할 수 있다.

과거에는 RNN(순환 신경망)을 통해서 자연어를 처리함. 같은 weight를 공유하고 데이터가 **순차적** 으로 들어감. 그러면 이전 데이터의 정보를 기반으로 새로 들어온 데이터를 분석할 수 있게 된다.

→ 그러나 문장이 길어질 수록, 처음에 들어온 단어에 대한 정보가 소실되는 문제가 있다. → 이를 보완하기 위해서 LSTM 을 통해서 가중치가 중요한 단어들이 정보가 오래 남도록 알고리즘(?)을 구성. 그러나 정보 소실의 완전한 해결을 이루지 못함. 

### 트랜스포머

RNN은 순차적으로 데이터가 들어가야 하기에 병렬적인 처리를 할 수 없다. 트랜스포머는 전체 데이터가 한 번에 들어가기에 병렬 처리가 가능하다!

트랜스포머가 좋은 이유는 Attention 에 있다. 이 Attention을 통해서 단어간의 관계, 단어 간의 연관성을 해결 할 수 있다. (Attention : 단어 사이의 관계, 연관성)

트랜스포머의 Q,K,V는 단어 임베딩으로 만들어지고, 이때는 단어의 순서, 위치를 알 수 없기에 Positional Encoding 을 통해서 단어의 순서? 를 알 수 있다.

기울기 소실과, 정보 손실을 보완하기 위해서 Add&Norm 과정을 거친다.

feed Forward는 일반적인 dense 레이어와 비슷한 것으로 학습을 심화하기 위해서 배치한다.


### 전이 학습

기존에 학습 된 모델을 기반으로 내가 사용하고자 하는 데이터 맞춰서 추가 학습을 진행하고 활용하는 방식.

#### 1. Feature-based 전이학습 (특징 추출 방식)

**Feature-based 전이학습**은 사전 학습된 모델을 **특징 추출기(Feature Extractor)**로 활용하는 방식입니다.

- **동작 방식:**
    1. 사전 학습된 모델의 **마지막 분류기(Classifier) 레이어**를 제거합니다.
    2. 나머지 모델의 모든 레이어(특징 추출기 부분)는 **고정(frozen)**시킵니다. 즉, 가중치를 업데이트하지 않습니다.
    3. 새로운 작업에 맞는 **새로운 분류기 레이어**를 추가하고, 이 새로운 레이어의 가중치만 학습시킵니다.
- **주요 특징:**
    - **학습 대상:** 새롭게 추가된 분류기 레이어만 학습합니다.
    - **장점:** 학습할 파라미터가 적어 **학습 속도가 빠르고, 계산 비용이 적습니다.** 새로운 데이터셋의 크기가 작을 때 **과적합(Overfitting) 위험이 낮습니다.**
    - **단점:** 사전 학습된 모델이 새로운 작업의 특징을 잘 표현하지 못하면 성능이 제한될 수 있습니다.


#### 2. Fine-tuning 전이학습 (미세 조정 방식)

**Fine-tuning 전이학습**은 사전 학습된 모델의 가중치를 새로운 작업에 맞게 **'미세하게 조정'**하는 방식입니다.

- **동작 방식:**
    1. 사전 학습된 모델의 마지막 레이어를 제거하고 새로운 레이어를 추가하는 것은 Feature-based 방식과 동일합니다.
    2. 하지만, 모델의 모든 레이어 또는 일부 **상위(나중) 레이어**의 가중치를 **고정하지 않고** 새로운 데이터셋으로 다시 학습시킵니다. 이때 일반적으로 아주 작은 학습률(learning rate)을 사용합니다.
- **주요 특징:**
    - **학습 대상:** 새롭게 추가된 레이어와 함께 사전 학습된 모델의 일부 또는 전체 레이어를 함께 학습합니다.
    - **장점:** 새로운 데이터셋에 모델을 더 잘 적응시켜 **더 높은 성능**을 기대할 수 있습니다.
    - **단점:** 학습할 파라미터가 많아 **학습 속도가 느리고, 계산 비용이 더 많이 듭니다.** 데이터셋이 충분히 크지 않으면 **과적합될 위험이 높습니다.**

### 실제 파인튜닝 하기

허깅페이스에서 모델과 데이터를 가져와서 기존 모델의 가중치를 새로운 데이터로 파인튜닝하기 

코랩 노트북:  8월 18일(광인사)-파인튜닝


## 3. 문제 해결 경험 (학습과정에서 직면했던 문제와 에러, 이를 어떻게 해결했는가)

트랜스포머는 어디서 학습이 일어나는가? 여러 인코더오 디코더가 쌓여서 있는데, 어디서 예측 값과 출력 값을 비교하고 역전파가 일어나는가? 그리고 무엇을 학습하고, 가중치는 어디에 적용되고 어디에 있는가?

마지막 디코더의 출력에서 손실(Loss)을 계산하고, 이 손실에 대한 오차 신호가 **모델 전체**, 즉 여러 겹으로 쌓인 **디코더와 인코더 모두**에게 역전파됩니다.

### 역전파가 전체 모델에 미치는 영향

역전파는 마치 강의 상류로 거슬러 올라가듯, 계산 그래프를 거꾸로 타고 올라가며 각 층의 가중치를 업데이트합니다.

1. **디코더 내부:** 마지막 디코더 블록에서 발생한 오차는 바로 아래 디코더 블록, 그리고 그 아래 디코더 블록으로 계속 전달됩니다. 이 과정에서 각 디코더 블록의 셀프 어텐션과 피드포워드 신경망의 가중치들이 업데이트됩니다.
2. **인코더로의 전파:** 디코더의 **크로스 어텐션(Cross-Attention) 레이어**를 통해 오차 신호가 인코더로 전달됩니다. 이 크로스 어텐션은 인코더의 출력(문맥 정보)을 참조하기 때문에, 디코더의 오차를 인코더의 가중치에 반영하여 업데이트합니다.


### 기울기 소실 문제에 대한 해결책

이러한 깊은 구조 때문에 말씀하신 **기울기 소실(Vanishing Gradient)** 문제는 매우 중요한 이슈입니다. 하지만 트랜스포머는 이를 효과적으로 해결하는 몇 가지 핵심적인 메커니즘을 사용합니다.

1. **잔차 연결(Residual Connections):** 트랜스포머의 모든 서브 레이어(어텐션, 피드포워드)는 입력값을 그대로 출력에 더해주는 잔차 연결을 가지고 있습니다. 이 연결은 기울기가 사라지지 않고 다음 층으로 직접 전달되는 **지름길**을 만들어 줍니다.
2. **레이어 정규화(Layer Normalization):** 각 서브 레이어의 출력 직후에 레이어 정규화를 적용하여 학습을 안정화시킵니다. 이는 학습 중에 활성화 값의 분포가 크게 변하는 것을 막아 기울기가 특정 값으로 치우치는 현상을 방지합니다.
3. **순환 구조의 부재:** RNN(순환 신경망)은 시퀀스의 길이가 길어질수록 기울기가 소실되거나 폭발하기 쉬운 **순환적인 구조**를 가집니다. 반면, 트랜스포머는 순환 구조 대신 모든 단어를 병렬로 처리하고 어텐션 메커니즘으로 관계를 파악하기 때문에, 근본적으로 기울기 소실 문제에 더 강한 면모를 보입니다.

결론적으로, 트랜스포머는 매우 깊은 구조를 가졌음에도 불구하고, **잔차 연결**과 같은 구조적 특징 덕분에 역전파가 효율적으로 일어나고 기울기 소실 문제를 효과적으로 완화합니다.

## 4. 참고 코드 / 예시

코랩 노트북:  8월 18일(광인사)-파인튜닝

## 5. 참고자료 / 링크


## 6. 느낀 점 / 회고 
저번주에 배운 내용을 복습하는 것으로 이해를 높일 수 있었다.
또한 허깅페이스에서 모델을 가져오고 우리의 데이터로 가중치를 약간 변형하는 파인튜닝 방식을 실습해 보았는데, 단어 임베딩과 토큰화가 자연어 처리에 매우 중요하다는 것을 배웠다.
마찬가지로 트랜스포머에 대해서도 이해가 잘 안 되었는데, 이번 주 다시 수업을 들으며 이해를 높일 수 있었다.