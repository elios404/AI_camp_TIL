---
layout: post
title: "8월 12일 화요일 TIL(Deep Learning)"
date: 2025-08-12 18:00:00 +0900
categories: July_week7
---

# 8월 12일 화요일 TIL 작성

## 1. 학습 주제
- 선형회귀, 로지스틱 회귀
- 퍼셉트론 / 다층 퍼셉트론
- 오차 역전파
- 활성화 함수
- 과적합
- K겹 교차 검증

## 2. 새롭게 알게된 점 / 핵심 개념 (가장 중요하다고 생각하는 개념)

### 선형회귀

데이터의 패턴을 찾아내는 것, 그리고 그것을 가장 잘 나타내는 직선을 찾아내는 것 → 최소 제곱법을 통해서 직선을 찾을 수 있다. 모든 데이터들에 대해서 오차가 최소가 되는 직선을 찾아야 한다.

평균 제곱 오차를 구하고 그것이 가장 작은 직선을 찾을 수 있다.

경사 하강법을 통해서 : 먼저 직선을 긋고 오차를 계산, 더 적은 오차를 향해서 나아가도록, 단 학습률에 따라서 오차를 어떻게 줄여나갈지, 오히려 늘어날지, 상황에 따라 다름

이걸 일일히 직접 할 수는 없다. → 텐서플로우, 케라스를 사용하면 쉽게 구할 수 있다.

### 선형회귀 with 텐서플로 (이게 맞게된 요약인가?)

경사하강법 → 옵티마이저?

최소제곱법 → 손실함수?

### 로지스틱 회귀

데이터에 대해서 분류를 할 때 사용하는 회귀함수?  0과 1 사이의 값을 예측한다.

**시그모이드 함수** : 로지스틱 회귀를 푸는 함수 → 결국 ax + b를 구한다.

### 로지스틱 회귀 with 텐서플로우

model.add 에서 activation = `sigmoid' 를 사용한다. 활성화 함수를 시그모이드를 사용

`model.compile` 에서 `optimizer=’sgd’`경사하강법 사용, `loss = ‘binary_crossentropy’` 선형회귀에서는 최소 오차 제곱을 사용하지만, 로지스틱 회귀는 binary_crossentory를 사용한다.

### 퍼셉트론

앞서 배운 로지스틱 회귀와 비슷하다. 이 회로는 입력값이 들어왔을 떄 활성화 함수에 의해서 일정 수준을 넘으면 true를 그렇지 않으면 false를 내보낸다. 즉 입력값의 역치에 따라서 활성화 여부가 결정된다.

입력 값을 여러개 받아, 출력을 만든느데, 입력값에 가중치를 조절할 수 있게 만들어서 학습을 한다. 그리고 그 입력과 가중치의 계산 결과인 가중 합을 바탕으로 활성화 함수를 통해서 참 거짓을 분류한다.

그러나 1개의 선으로는 XOR 문제를 풀 수 없다. → 그렇다면? 여러개의 선을 통해서 분류를 한다.(**다층 퍼셉트론), 은닉층을 통해, 선을 여러개 만드는 것으로 해결**

### 다층 퍼셉트론

여러 층과 가중치가 있지만, 가중치를 a, 편향(bias)를 b로 하여서 다층 퍼셉트론도 ax + b라고 볼 수 있다.

### 딥러닝과 오차 역전파

오차 역전파를 통해서 숨겨진 은닉층의 가중치를 업데이트 할 수 있다. 가중치와 편향으로 나온 예측 값을 실제 결과와 비교한 뒤 그것을 수정하기 위해서 다시 **뒤쪽** 으로 가중치와 편형 값을 업데이트 하는 것이 오차 역전파이다.

질문 : 경사하강법은 학습률에 따라서 오차가 늘어날 수도 있다. 그럼 딥러닝도 같은 문제가 있는가?

대답 :  마찬가지로 가지고 있다. 학습률이 너무 크면, 발산할 수 있고, 작으면 너무 오래걸리거나, 지역 최저점에서 머물러, 실제 최저점에 갈 수 없을 수 있다.  따라서 옵티마이저를 통해서 학습 과정에서 학습률을 상황에 맞게 변경시켜주는 방법이 있다.

역전파 알고리즘은 은닉층을 통해서 하나의 가중치가 영향을 주는 노드의 수가 기하급수적으로 늘어나기에 계산하기 매우 복잡해진다.

### 활성화 함수

**다층 퍼셉트론은 활성함수를 통해서 선형으로 예측하는 것을 비선형으로 바꿔준다.** → 이해가 잘 안됨, 왜 활성함수가? 활성함수는 0,1로 분류하는 함수로 생각할 수 있는 것 아님?

```
활성함수가 비선형성을 만드는 이유
다층 퍼셉트론(Multi-Layer Perceptron, MLP)은 여러 개의 레이어(Layer)를 쌓아 올린 구조입니다. 각 레이어는 입력에 대해 가중치(Weight)를 곱하고 편향(Bias)을 더하는 선형 변환(y = Wx + b)을 수행합니다.

만약 활성함수가 없다면, 모든 레이어는 결국 단순히 선형 변환만 반복하게 됩니다. 여러 번의 선형 변환을 거쳐도 결과는 여전히 하나의 큰 선형 변환과 같습니다. 예를 들어, 두 번의 선형 변환 y = W2(W1x + b1) + b2를 전개하면 y = (W2W1)x + (W2b1 + b2)가 되어, 결국 y = W_new * x + b_new라는 하나의 선형 방정식으로 압축됩니다.

이런 경우, 다층 퍼셉트론은 층을 아무리 많이 쌓아도 하나의 선형 모델(단층 퍼셉트론)과 같은 능력밖에 가지지 못합니다. 즉, 데이터의 복잡한 비선형 관계를 학습할 수 없게 됩니다.

활성함수는 각 레이어의 출력에 적용되어 이 선형 변환의 결과에 비선형성을 추가합니다. 이 비선형 변환 덕분에 각 레이어는 더 복잡한 패턴을 인식하고, 다층 구조를 통해 선형 모델로는 해결할 수 없는 문제들을 해결할 수 있게 됩니다.

활성함수는 0 또는 1로만 분류하는 함수인가요?
활성함수가 0과 1로만 분류하는 함수라는 생각은 시그모이드(Sigmoid) 함수와 관련이 있습니다. 시그모이드 함수는 출력을 0과 1 사이의 값으로 압축하는 특징 때문에, 주로 이진 분류(Binary Classification) 문제의 마지막 출력 레이어에서 사용됩니다. 여기서 0.5를 기준으로 0 또는 1로 분류하는 기준을 만들 수 있습니다.

하지만, **히든 레이어(Hidden Layer)**에서는 시그모이드 외에도 다양한 활성함수가 사용되며, 이들의 주된 목적은 비선형성을 도입하는 것입니다.

ReLU(Rectified Linear Unit): 입력이 0보다 작으면 0, 크면 그대로 출력하는 함수로, 딥러닝에서 가장 널리 사용됩니다.

Tanh(하이퍼볼릭 탄젠트): 출력을 -1과 1 사이로 압축하는 함수입니다.

결론적으로, 활성함수는 딥러닝이 복잡한 세상의 비선형적 관계를 모델링할 수 있도록 만들어주는 핵심 요소입니다. 시그모이드처럼 특정 용도로 사용되는 함수도 있지만, 그 본질적인 역할은 비선형성을 부여하는 것입니다.
```

**기울기 소실 문제와 활성화 함수** → 시그모이드 활성화 함수를 미분하면 그 값이 0에 가까워진다. 그럼 몇 차례 은닉층을 지나면 기울기가 0이된다. 기울기를 바탕으로 가중치를 수정할 수 없어진다.

기울기 소실 문제를 해결하기 위해서 ReLU 라는 새로운 활성화 함수를 제안 → 현재 사용중인 활성화함수

### 고급 경사하강법

경사하강법의 속도와 정확도 문제를 해결하기 위해서 → 확률적 경사 하강법(속도의 향상) + 모멘텀을 적용해서(정확도의 향상) → 최종적으로 **아담(Adam)** 이라는 경사하강법을 사용한다.

### 입력층, 은닉층, 출력층

colab을 통한 실습

### 원-핫 인코딩

여러 개의 분류(문자열 등)로 나누어져 있는 숫자로 나타내는 것, 각 분류에 대해서 새로운 열을 만들고 해당되는 열에만 1로 처리하고 나머지를 0으로 `get_dummies(y)` 등을 사용해서 원 핫 인코딩 가능

만약 true, false로 결과가 나온다면 `y.astype(int)`  를 통해서 0,1의 int 값으로 바꾸어주기

### 과적합

학습한 데이터에 대해서는 정확도가 높지만, 새로운 데이터에 적용하면 잘 맞지 않는 상황, 즉 일반적인 상황의 데이터가 아니라, 학습한 데이터에 대해서만 너무 높은 적합도를 가지는 것

은닉층이나 노드가 너무 많거나, 변수가 너무 복잡해서 발생하기도 하고, 테스트 셋과 학습 셋이 중복 될 때, 학습epoch이 너무 많을 때 등등 발생할 수 있다.

→ 따라서 학습데이터와 테스트 데이터를 나누어서 테스트 데이터로 평가해야 한다.

```python
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, shuffle=True)
```

### K겹 교차 검증

학습 데이터 중 일부를 검증 데이터로 사용하면, 학습 데이터의 양이 줄어들음 + 검증 데이터는 학습 데이터로 사용하지 못함. 따라서 100%로 학습 데이터로 사용하기 위해서 K겹 교차 검증을 활용한다.

## 3. 문제 해결 경험 (학습과정에서 직면했던 문제와 에러, 이를 어떻게 해결했는가)


## 4. 참고 코드 / 예시
코랩 : 8월12일-광인사(딥러닝) 파일을 통해서 확인가능

## 5. 참고자료 / 링크


## 6. 느낀 점 / 회고 
K겹 교차검증이 이전에 데이터 분석할 때 중요하다는 것을 느꼈었는데, 이를 잘 활용할 수 있어야 한다고 느낌.