---
layout: post
title: "8월 14일 목요일 TIL(RNN, Transformer)"
date: 2025-08-14 18:00:00 +0900
categories: July_week7
---

# 8월 14일 목요일 TIL 작성

## 1. 학습 주제
- RNN
- Transformer
  - self-attention
  - Q,K,V
  - 인코딩 / 디코딩 부분
- 전이 학습


## 2. 새롭게 알게된 점 / 핵심 개념 (가장 중요하다고 생각하는 개념)

### 문장의 감정 예측하기

colab 자연어 처리 코드 확인하기 → 질문 거리 해결하기

### RNN

- 시퀀스 데이터 특화
- ‘기억’ 능력을 가지고 있음
- 입력을 모두 처리하고 난 후 네트워크에 남겨진 기억은 전체 시퀀스 정보를 요약해서 저장
- 이 과정은 새로운 단어가 추가될 때 마다 계속 반복(**순환적**)

초기의 RNN은 단계가 진행될 수록, 처음에 들어온 입력에 대한 기억, 영향력이 문장이 길어질 수록 적어진다는 문제가 있다.

→ LSTM(Long Short-Term Memory)를 통해서 중요하다고 생각하는 단어를 중간에 지속적으로 입력으로 넣어준다.

또한 추가적인 단점을 문장이 단어별로 순차적으로 입력되어야 하기에 병렬 처리가 안되고 속도가 느리다. → **트랜스포머** 는 모든 단어가 한 번에 들어가고 병렬적으로 처리 가능하다.

### Transformer(트랜스포머)

수업시간에 배운 트랜스포머로는 부족하다. → 따로 공부가 필요함.

트랜스포머는 자연어를 **병렬처리**하는 것에 강점이 있다!

단어 간의 관계를 통해서 문맥을 이해한다.

어텐션(Attention)은 **문장 내에서 가장 중요한 단어에 집중하여 문맥을 이해하는 기술**

### Self-Attention(Intra-Attention)

자기 자신에 대해서 attention을 수행한다. → 문장 내 단어들의 유사도를 구한다. (문장의 문맥을 파악하는데 도움)

3가지

- Query Vector
- Key Vector
- Value Vector

### 인코딩 부분 단계

- Multi-Head Attention : 최종적으로 attention value를 구한다.
- Add & Norm :  기울기 소실을 막기 위해서 중간에 잔차로, 데이터를 넣어준다.
- Feed Forward : 은닉층의 역할을 한다.
- Positionial Emcoding : 문장이 한 번에 전체가 들어가기에, 각 단어의 순서를 파악할 수 있는 임베딩을 한 번 더 진행한다.

### 디코딩 부분 단계

- Masked Multi-Head Layer :  단어가 생성되기에 현재 단어는 뒤 쪽 단어를 알 수 없음. 모르는 부분은 마스킹 한다.


### 전이 학습 방법

- feature-based transfer learning : pre-trained model의 출력 feature를 new model의 입력으로 사용, 즉 이미 있는 모델의 출력을 데이터 삼아서 새로운 모델의 학습 데이터로 사용, 이때 모델은 새로운 모델 생성/훈련을 한다.
- fine-Tuning : pre-trained weight 를 초기값으로 사용하여 동일한 모델을 fine-tuning, 즉 동일한 모델을 사용하여 다른 데이터를 통해서 모델의 가중치를 나의 데이터에 맞게 조금씩 수정하는 것

### 소규모 데이터 셋일 때 성능 향상

이미지를 확대, 축소, 반전 등을 통해서 이미지의 양을 증가 시킬 수 있다.


## 3. 문제 해결 경험 (학습과정에서 직면했던 문제와 에러, 이를 어떻게 해결했는가)


### 📝 TIL: Self-Attention과 Q, K, V의 원리

딥러닝의 핵심 개념 중 하나인 **Self-Attention**과 그 구성 요소인 **Q(Query), K(Key), V(Value)**에 대해 깊이 있게 학습했다. 내가 처음에 가졌던 생각과 여러 질문들을 통해 개념을 명확히 정리할 수 있었다.

---

#### 1. 내가 알았던 것 (초기 이해)

- **Self-Attention**은 문장 내의 단어들끼리 서로의 유사도를 계산해, 문맥을 파악하는 메커니즘이다.
- 이 과정에서 **Q, K, V**라는 세 가지 벡터가 사용된다.

#### 2. 처음 궁금했던 점 (Q, K, V의 역할)

- **Q, K, V가 각각 무엇을 의미하는가?** 단순히 3가지 벡터로 나뉜다는 것을 넘어, 각 벡터의 구체적인 역할이 궁금했다.

**설명 (이해 과정):**
도서관에서 책을 찾는 과정으로 비유해 쉽게 이해했다.

- **Query(Q):** "지금 내가 찾는 내용은 무엇인가?"와 같은 질문의 역할.
- **Key(K):** "이 책은 이런 키워드(Key)를 가지고 있습니다."와 같이 모든 단어의 특징을 담은 꼬리표 역할.
- **Value(V):** "책의 실제 내용(Value)은 이렇습니다."와 같이 단어의 의미적 정보를 담고 있는 역할.
- Q와 K를 비교해 유사도 점수(가중치)를 얻고, 그 가중치를 V에 곱해 문맥이 반영된 새로운 단어 벡터를 얻는 과정이다.

#### 3. 핵심 의문점과 최종 정리

위의 과정을 통해 원리를 이해했지만, 그 값들이 어디서 오고, 어떻게 계산되는지에 대한 더 깊은 의문이 생겼다.

- **Q, K, V 벡터의 값은 어디서 오는가?**
    - 단순히 랜덤한 값이 아니라, 모든 단어의 출발점인 **하나의 워드 임베딩 벡터**에서 시작된다는 것을 알게 되었다.
    - 이 임베딩 벡터가 **WQ,WK,WV라는 세 개의 다른 가중치 행렬**과 곱해져서 Q, K, V라는 세 가지 다른 벡터로 변환된다. 즉, 값 자체가 처음부터 같은 것은 아니다.
- **그렇다면 그 가중치 행렬은 어디서 오는가?**
    - 이 가중치 행렬들은 정해져 있는 것이 아니라, 딥러닝 모델이 **학습을 통해 스스로 찾아내는** 파라미터라는 것을 알았다.
    - 처음에는 무작위 값으로 초기화되지만, 학습 과정에서 **역전파(Backpropagation)**를 통해 손실을 줄이는 방향으로 조금씩 수정된다.
- **최종 학습 과정 정리:**
    1. WQ,WK,WV를 포함한 모든 가중치 행렬을 무작위로 초기화한다.
    2. 입력 문장의 단어 임베딩이 이 가중치 행렬을 통과하며 Q, K, V 벡터를 만든다.
    3. Q, K의 **내적**을 통해 단어 간의 유사도를 계산하고, 이를 기반으로 V를 가중치 합산하여 문맥이 반영된 예측값을 만든다.
    4. 예측값과 실제 정답을 비교하여 오차(손실)를 계산한다.
    5. **역전파**를 통해 오차를 줄이는 방향으로 WQ,WK,WV 등의 가중치 행렬을 업데이트한다.

결론적으로, **트랜스포머의 학습은 Q, K, V 벡터를 가장 효과적으로 만들어낼 수 있는 가중치 행렬을 찾아가는 과정**이라는 것을 명확히 이해하게 되었다.


## 4. 참고 코드 / 예시
코랩
- 광인사(8.13~14) - 딥러닝, 자연어 처리 : 문장 감정 분석 부분
- Transformer 코드 / Transformer.ipynb
- ch18, ch18-colab, ch21

## 5. 참고자료 / 링크
https://www.youtube.com/watch?v=g38aoGttLhI

## 6. 느낀 점 / 회고 
트랜스포머에 대한 내용은 대학 수업에서 들은 기억이 있지만, 그때도 이해가 잘 되지 않았었다. 그러나 이번에도 시간적 여유가 없어서 그런지, 오늘은 더욱 빠르게 진도를 나가서 대부분의 내용을 이해하기 어려웠다. 그마나. Query, key, value 에 대한 부분은 gpt를 통해서 추가학습을 하여 어느정도 이해를 했다.